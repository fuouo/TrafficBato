{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.regression import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dbn.tensorflow import SupervisedDBNRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM_EPOCHS = 5\n",
    "DBN_EPOCHS = 150\n",
    "RBM_LEARNING_RATE = 0.01\n",
    "DBN_LEARNING_RATE = 0.01\n",
    "HIDDEN_LAYER_STRUCT = [20, 50, 100]\n",
    "ACTIVE_FUNC = 'relu'\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Road and Year of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "ROAD = \"Quirino\"\n",
    "YEAR = \"2015\"\n",
    "EXT = \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Traffic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statusN</th>\n",
       "      <th>statusS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   statusN  statusS\n",
       "0      0.0      0.0\n",
       "1      0.0      0.0\n",
       "2      0.0      0.0\n",
       "3      0.0      0.0\n",
       "4      0.0      0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAFFIC_WINDOWSIZE = 4\n",
    "TRAFFIC_FILENAME = \"eng_win\" + str(TRAFFIC_WINDOWSIZE) + \"_mmda_\" + ROAD + \"_\" + YEAR\n",
    "#TRAFFIC_FILENAME = \"noeng_mmda_\" + ROAD + \"_\" + YEAR +\"_transformed\"\n",
    "\n",
    "traffic_raw_data = pd.read_csv(\"data/mmda/\" + TRAFFIC_FILENAME + EXT, skipinitialspace=True)\n",
    "traffic_raw_data = traffic_raw_data.fillna(0)\n",
    "traffic_dataset = traffic_raw_data\n",
    "\n",
    "# Remove date time. Remove unused columms\n",
    "#0-2 = dt + lineName + stationName || 3-4 - statusN - statusS || 5-14 - original weather variables\n",
    "#15-46 - engineered traffic\n",
    "cols_to_remove = [0, 1, 2]\n",
    "\n",
    "# window 1\n",
    "statusN = list(range(5, 9))\n",
    "statusS = list(range(12, 16))\n",
    "\n",
    "cols_to_remove += statusN + statusS\n",
    "\n",
    "# window >= 2\n",
    "statusN2 = list(range(9, 12))\n",
    "statusS2 = list(range(16, 19))\n",
    "\n",
    "cols_to_remove += statusN2 + statusS2\n",
    "\n",
    "#cols_to_remove += [3, 4] #statusN , statusS\n",
    "\n",
    "traffic_dataset.drop(traffic_dataset.columns[[cols_to_remove]], axis=1, inplace=True)\n",
    "traffic_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Traffic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempC</th>\n",
       "      <th>windspeedKmph</th>\n",
       "      <th>tempC_Rmean (window = 9)</th>\n",
       "      <th>tempC_Rmin  (window = 9)</th>\n",
       "      <th>tempC_Rmax  (window = 9)</th>\n",
       "      <th>windspeedKmph_Rmean (window = 9)</th>\n",
       "      <th>windspeedKmph_Rmin  (window = 9)</th>\n",
       "      <th>windspeedKmph_Rmax  (window = 9)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tempC  windspeedKmph  tempC_Rmean (window = 9)  tempC_Rmin  (window = 9)  \\\n",
       "0  0.2000       0.295455                       0.0                       0.0   \n",
       "1  0.1875       0.295455                       0.0                       0.0   \n",
       "2  0.1750       0.295455                       0.0                       0.0   \n",
       "3  0.1625       0.295455                       0.0                       0.0   \n",
       "4  0.1500       0.295455                       0.0                       0.0   \n",
       "\n",
       "   tempC_Rmax  (window = 9)  windspeedKmph_Rmean (window = 9)  \\\n",
       "0                       0.0                               0.0   \n",
       "1                       0.0                               0.0   \n",
       "2                       0.0                               0.0   \n",
       "3                       0.0                               0.0   \n",
       "4                       0.0                               0.0   \n",
       "\n",
       "   windspeedKmph_Rmin  (window = 9)  windspeedKmph_Rmax  (window = 9)  \n",
       "0                               0.0                               0.0  \n",
       "1                               0.0                               0.0  \n",
       "2                               0.0                               0.0  \n",
       "3                               0.0                               0.0  \n",
       "4                               0.0                               0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEATHER_WINDOWSIZE = 9\n",
    "WEATHER_FILENAME = \"eng_win\" + str(WEATHER_WINDOWSIZE) + \"_wwo_\" + YEAR\n",
    "weather_dataset = pd.read_csv(\"data/wwo/\" + WEATHER_FILENAME + EXT, skipinitialspace=True)\n",
    "weather_dataset = weather_dataset.fillna(0)\n",
    "\n",
    "cols_to_remove = [0, 1, 2] \n",
    "\n",
    "#For Window = >2\n",
    "temp = list(range(13, 17))\n",
    "temp2 = list(range(17, 20))\n",
    "windspeedkmph = list(range(20, 24))\n",
    "windspeedkmph2 = list(range(24, 27))\n",
    "cond = list(range(27, 31))\n",
    "cond2 = list(range(31, 34))\n",
    "precip = list(range(34, 38))\n",
    "precip2 = list(range(38, 41))\n",
    "humid = list(range(41, 45))\n",
    "humid2 = list(range(45, 48))\n",
    "visibility = list(range(48, 52))\n",
    "visibility2 = list(range(52, 55))\n",
    "pressure = list(range(55, 59))\n",
    "pressure2 = list(range(59, 62))\n",
    "cloudcover = list(range(62, 66))\n",
    "cloudcover2 = list(range(66, 69))\n",
    "dewpoint = list(range(69, 73))\n",
    "dewpoint2 = list(range(73, 76))\n",
    "windgustkmph = list(range(76, 80))\n",
    "windgustkmph2 = list(range(80, 83))\n",
    "\n",
    "#Expanding Window.\n",
    "# All Expanding Window Columns\n",
    "cols_to_remove += temp + windspeedkmph + cond + precip + humid + visibility + pressure +  cloudcover + dewpoint + windgustkmph #Window = 1\n",
    "\n",
    "#Window  >=2\n",
    "cols_to_remove += cond2 + precip2 + humid2 + visibility2 + pressure2 + cloudcover2 + dewpoint2 + windgustkmph2\n",
    "# All rolling window columns\n",
    "#cols_to_remove += temp2 + windspeedkmph2 + cond2 + precip2 + humid2 + visibility2 + pressure2 + cloudcover2 + dewpoint2 + windgustkmph2\n",
    "\n",
    "#cols_to_remove += [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # All Original Weather Variables\n",
    "\n",
    "weather_dataset = weather_dataset.drop(weather_dataset.columns[[cols_to_remove]], axis=1)\n",
    "weather_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WL [El.m]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WL [El.m]\n",
       "0   0.814856\n",
       "1   0.814856\n",
       "2   0.814856\n",
       "3   0.814856\n",
       "4   0.814856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLOOD_WINDOWSIZE = 2\n",
    "FLOOD_FILENAME = \"eng_win\" + str(FLOOD_WINDOWSIZE) + \"_flood_\" + YEAR\n",
    "flood_raw_data = pd.read_csv(\"data/flood/\" + FLOOD_FILENAME + EXT, skipinitialspace=True)\n",
    "flood_raw_data = flood_raw_data.fillna(0)\n",
    "\n",
    "cols_to_remove = [0]\n",
    "\n",
    "flood = [2, 3, 4, 5]\n",
    "cols_to_remove += flood\n",
    "\n",
    "flood2 = list(range(6, 9))\n",
    "cols_to_remove += flood2\n",
    "\n",
    "flood_dataset = flood_raw_data\n",
    "flood_dataset = flood_dataset.drop(flood_dataset.columns[cols_to_remove], axis=1)\n",
    "flood_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempC</th>\n",
       "      <th>windspeedKmph</th>\n",
       "      <th>tempC_Rmean (window = 9)</th>\n",
       "      <th>tempC_Rmin  (window = 9)</th>\n",
       "      <th>tempC_Rmax  (window = 9)</th>\n",
       "      <th>windspeedKmph_Rmean (window = 9)</th>\n",
       "      <th>windspeedKmph_Rmin  (window = 9)</th>\n",
       "      <th>windspeedKmph_Rmax  (window = 9)</th>\n",
       "      <th>WL [El.m]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tempC  windspeedKmph  tempC_Rmean (window = 9)  tempC_Rmin  (window = 9)  \\\n",
       "0  0.2000       0.295455                       0.0                       0.0   \n",
       "1  0.1875       0.295455                       0.0                       0.0   \n",
       "2  0.1750       0.295455                       0.0                       0.0   \n",
       "3  0.1625       0.295455                       0.0                       0.0   \n",
       "4  0.1500       0.295455                       0.0                       0.0   \n",
       "\n",
       "   tempC_Rmax  (window = 9)  windspeedKmph_Rmean (window = 9)  \\\n",
       "0                       0.0                               0.0   \n",
       "1                       0.0                               0.0   \n",
       "2                       0.0                               0.0   \n",
       "3                       0.0                               0.0   \n",
       "4                       0.0                               0.0   \n",
       "\n",
       "   windspeedKmph_Rmin  (window = 9)  windspeedKmph_Rmax  (window = 9)  \\\n",
       "0                               0.0                               0.0   \n",
       "1                               0.0                               0.0   \n",
       "2                               0.0                               0.0   \n",
       "3                               0.0                               0.0   \n",
       "4                               0.0                               0.0   \n",
       "\n",
       "   WL [El.m]  \n",
       "0   0.814856  \n",
       "1   0.814856  \n",
       "2   0.814856  \n",
       "3   0.814856  \n",
       "4   0.814856  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dataset = pd.concat([weather_dataset, flood_dataset], axis=1)\n",
    "weather_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Training PM1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset for PM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-be Predicted variable \n",
    "Y = traffic_dataset.statusS\n",
    "Y = Y.shift(-shift)\n",
    "Y = Y.fillna(0)\n",
    "Y = Y.round(5)\n",
    "Y = Y[:-shift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other data\n",
    "X = traffic_dataset [:-shift]\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.67, shuffle=False)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Data scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.083082\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.039274\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.032054\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.028211\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.028537\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.091676\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.050343\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.024477\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.017136\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.013503\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.078696\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.051939\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.025084\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.013016\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.011979\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.018365\n",
      ">> Epoch 1 finished \tANN training loss 0.013482\n",
      ">> Epoch 2 finished \tANN training loss 0.011962\n",
      ">> Epoch 3 finished \tANN training loss 0.011346\n",
      ">> Epoch 4 finished \tANN training loss 0.010995\n",
      ">> Epoch 5 finished \tANN training loss 0.010807\n",
      ">> Epoch 6 finished \tANN training loss 0.010708\n",
      ">> Epoch 7 finished \tANN training loss 0.010651\n",
      ">> Epoch 8 finished \tANN training loss 0.010621\n",
      ">> Epoch 9 finished \tANN training loss 0.010602\n",
      ">> Epoch 10 finished \tANN training loss 0.010594\n",
      ">> Epoch 11 finished \tANN training loss 0.010596\n",
      ">> Epoch 12 finished \tANN training loss 0.010597\n",
      ">> Epoch 13 finished \tANN training loss 0.010573\n",
      ">> Epoch 14 finished \tANN training loss 0.010572\n",
      ">> Epoch 15 finished \tANN training loss 0.010558\n",
      ">> Epoch 16 finished \tANN training loss 0.010556\n",
      ">> Epoch 17 finished \tANN training loss 0.010556\n",
      ">> Epoch 18 finished \tANN training loss 0.010556\n",
      ">> Epoch 19 finished \tANN training loss 0.010591\n",
      ">> Epoch 20 finished \tANN training loss 0.010558\n",
      ">> Epoch 21 finished \tANN training loss 0.010552\n",
      ">> Epoch 22 finished \tANN training loss 0.010567\n",
      ">> Epoch 23 finished \tANN training loss 0.010573\n",
      ">> Epoch 24 finished \tANN training loss 0.010557\n",
      ">> Epoch 25 finished \tANN training loss 0.010600\n",
      ">> Epoch 26 finished \tANN training loss 0.010554\n",
      ">> Epoch 27 finished \tANN training loss 0.010551\n",
      ">> Epoch 28 finished \tANN training loss 0.010606\n",
      ">> Epoch 29 finished \tANN training loss 0.010554\n",
      ">> Epoch 30 finished \tANN training loss 0.010561\n",
      ">> Epoch 31 finished \tANN training loss 0.010550\n",
      ">> Epoch 32 finished \tANN training loss 0.010555\n",
      ">> Epoch 33 finished \tANN training loss 0.010567\n",
      ">> Epoch 34 finished \tANN training loss 0.010551\n",
      ">> Epoch 35 finished \tANN training loss 0.010593\n",
      ">> Epoch 36 finished \tANN training loss 0.010556\n",
      ">> Epoch 37 finished \tANN training loss 0.010589\n",
      ">> Epoch 38 finished \tANN training loss 0.010576\n",
      ">> Epoch 39 finished \tANN training loss 0.010555\n",
      ">> Epoch 40 finished \tANN training loss 0.010549\n",
      ">> Epoch 41 finished \tANN training loss 0.010623\n",
      ">> Epoch 42 finished \tANN training loss 0.010555\n",
      ">> Epoch 43 finished \tANN training loss 0.010558\n",
      ">> Epoch 44 finished \tANN training loss 0.010550\n",
      ">> Epoch 45 finished \tANN training loss 0.010584\n",
      ">> Epoch 46 finished \tANN training loss 0.010583\n",
      ">> Epoch 47 finished \tANN training loss 0.010552\n",
      ">> Epoch 48 finished \tANN training loss 0.010597\n",
      ">> Epoch 49 finished \tANN training loss 0.010547\n",
      ">> Epoch 50 finished \tANN training loss 0.010576\n",
      ">> Epoch 51 finished \tANN training loss 0.010565\n",
      ">> Epoch 52 finished \tANN training loss 0.010556\n",
      ">> Epoch 53 finished \tANN training loss 0.010679\n",
      ">> Epoch 54 finished \tANN training loss 0.010579\n",
      ">> Epoch 55 finished \tANN training loss 0.010565\n",
      ">> Epoch 56 finished \tANN training loss 0.010550\n",
      ">> Epoch 57 finished \tANN training loss 0.010550\n",
      ">> Epoch 58 finished \tANN training loss 0.010582\n",
      ">> Epoch 59 finished \tANN training loss 0.010549\n",
      ">> Epoch 60 finished \tANN training loss 0.010639\n",
      ">> Epoch 61 finished \tANN training loss 0.010576\n",
      ">> Epoch 62 finished \tANN training loss 0.010577\n",
      ">> Epoch 63 finished \tANN training loss 0.010662\n",
      ">> Epoch 64 finished \tANN training loss 0.010561\n",
      ">> Epoch 65 finished \tANN training loss 0.010554\n",
      ">> Epoch 66 finished \tANN training loss 0.010575\n",
      ">> Epoch 67 finished \tANN training loss 0.010547\n",
      ">> Epoch 68 finished \tANN training loss 0.010558\n",
      ">> Epoch 69 finished \tANN training loss 0.010562\n",
      ">> Epoch 70 finished \tANN training loss 0.010547\n",
      ">> Epoch 71 finished \tANN training loss 0.010548\n",
      ">> Epoch 72 finished \tANN training loss 0.010547\n",
      ">> Epoch 73 finished \tANN training loss 0.010560\n",
      ">> Epoch 74 finished \tANN training loss 0.010546\n",
      ">> Epoch 75 finished \tANN training loss 0.010567\n",
      ">> Epoch 76 finished \tANN training loss 0.010600\n",
      ">> Epoch 77 finished \tANN training loss 0.010560\n",
      ">> Epoch 78 finished \tANN training loss 0.010545\n",
      ">> Epoch 79 finished \tANN training loss 0.010545\n",
      ">> Epoch 80 finished \tANN training loss 0.010546\n",
      ">> Epoch 81 finished \tANN training loss 0.010548\n",
      ">> Epoch 82 finished \tANN training loss 0.010633\n",
      ">> Epoch 83 finished \tANN training loss 0.010547\n",
      ">> Epoch 84 finished \tANN training loss 0.010581\n",
      ">> Epoch 85 finished \tANN training loss 0.010558\n",
      ">> Epoch 86 finished \tANN training loss 0.010556\n",
      ">> Epoch 87 finished \tANN training loss 0.010600\n",
      ">> Epoch 88 finished \tANN training loss 0.010593\n",
      ">> Epoch 89 finished \tANN training loss 0.010558\n",
      ">> Epoch 90 finished \tANN training loss 0.010552\n",
      ">> Epoch 91 finished \tANN training loss 0.010548\n",
      ">> Epoch 92 finished \tANN training loss 0.010560\n",
      ">> Epoch 93 finished \tANN training loss 0.010547\n",
      ">> Epoch 94 finished \tANN training loss 0.010545\n",
      ">> Epoch 95 finished \tANN training loss 0.010547\n",
      ">> Epoch 96 finished \tANN training loss 0.010603\n",
      ">> Epoch 97 finished \tANN training loss 0.010580\n",
      ">> Epoch 98 finished \tANN training loss 0.010553\n",
      ">> Epoch 99 finished \tANN training loss 0.010551\n",
      ">> Epoch 100 finished \tANN training loss 0.010558\n",
      ">> Epoch 101 finished \tANN training loss 0.010554\n",
      ">> Epoch 102 finished \tANN training loss 0.010558\n",
      ">> Epoch 103 finished \tANN training loss 0.010599\n",
      ">> Epoch 104 finished \tANN training loss 0.010595\n",
      ">> Epoch 105 finished \tANN training loss 0.010543\n",
      ">> Epoch 106 finished \tANN training loss 0.010562\n",
      ">> Epoch 107 finished \tANN training loss 0.010542\n",
      ">> Epoch 108 finished \tANN training loss 0.010544\n",
      ">> Epoch 109 finished \tANN training loss 0.010544\n",
      ">> Epoch 110 finished \tANN training loss 0.010576\n",
      ">> Epoch 111 finished \tANN training loss 0.010547\n",
      ">> Epoch 112 finished \tANN training loss 0.010544\n",
      ">> Epoch 113 finished \tANN training loss 0.010559\n",
      ">> Epoch 114 finished \tANN training loss 0.010542\n",
      ">> Epoch 115 finished \tANN training loss 0.010556\n",
      ">> Epoch 116 finished \tANN training loss 0.010797\n",
      ">> Epoch 117 finished \tANN training loss 0.010554\n",
      ">> Epoch 118 finished \tANN training loss 0.010555\n",
      ">> Epoch 119 finished \tANN training loss 0.010579\n",
      ">> Epoch 120 finished \tANN training loss 0.010546\n",
      ">> Epoch 121 finished \tANN training loss 0.010542\n",
      ">> Epoch 122 finished \tANN training loss 0.010576\n",
      ">> Epoch 123 finished \tANN training loss 0.010550\n",
      ">> Epoch 124 finished \tANN training loss 0.010558\n",
      ">> Epoch 125 finished \tANN training loss 0.010572\n",
      ">> Epoch 126 finished \tANN training loss 0.010566\n",
      ">> Epoch 127 finished \tANN training loss 0.010557\n",
      ">> Epoch 128 finished \tANN training loss 0.010572\n",
      ">> Epoch 129 finished \tANN training loss 0.010542\n",
      ">> Epoch 130 finished \tANN training loss 0.010544\n",
      ">> Epoch 131 finished \tANN training loss 0.010549\n",
      ">> Epoch 132 finished \tANN training loss 0.010587\n",
      ">> Epoch 133 finished \tANN training loss 0.010574\n",
      ">> Epoch 134 finished \tANN training loss 0.010548\n",
      ">> Epoch 135 finished \tANN training loss 0.010572\n",
      ">> Epoch 136 finished \tANN training loss 0.010558\n",
      ">> Epoch 137 finished \tANN training loss 0.010608\n",
      ">> Epoch 138 finished \tANN training loss 0.010556\n",
      ">> Epoch 139 finished \tANN training loss 0.010542\n",
      ">> Epoch 140 finished \tANN training loss 0.010548\n",
      ">> Epoch 141 finished \tANN training loss 0.010544\n",
      ">> Epoch 142 finished \tANN training loss 0.010547\n",
      ">> Epoch 143 finished \tANN training loss 0.010546\n",
      ">> Epoch 144 finished \tANN training loss 0.010550\n",
      ">> Epoch 145 finished \tANN training loss 0.010544\n",
      ">> Epoch 146 finished \tANN training loss 0.010611\n",
      ">> Epoch 147 finished \tANN training loss 0.010542\n",
      ">> Epoch 148 finished \tANN training loss 0.010547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 149 finished \tANN training loss 0.010548\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNRegression(batch_size=16, dropout_p=0, l2_regularization=1.0,\n",
       "            learning_rate=0.01, n_iter_backprop=150, verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm1 = SupervisedDBNRegression(hidden_layers_structure=HIDDEN_LAYER_STRUCT,\n",
    "                                    learning_rate_rbm=RBM_LEARNING_RATE,\n",
    "                                    learning_rate=DBN_LEARNING_RATE,\n",
    "                                    n_epochs_rbm=RBM_EPOCHS,\n",
    "                                    n_iter_backprop=DBN_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    activation_function=ACTIVE_FUNC)\n",
    "pm1.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "R-squared: 0.924729\n",
      "MSE: 0.006026\n",
      "MAE: 0.017608\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "Y_pred = pm1.predict(X_test)\n",
    "print('Done.\\nR-squared: %f\\nMSE: %f' % (r2_score(Y_test, Y_pred), mean_squared_error(Y_test, Y_pred)))\n",
    "print('MAE: %f' %(mean_absolute_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling Results\n",
    "temp = []\n",
    "for i in range(len(Y_pred)):\n",
    "    temp.append(Y_pred[i][0])\n",
    "d = {'Predicted': temp, 'Actual': Y_test}\n",
    "pm1_results = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Results into csv file\n",
    "pm1_results.to_csv(\"output/pm1_output_\" + ROAD + \"_\" + YEAR + EXT, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm1.save('models/pm1_' + ROAD + '_' + YEAR +'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Training PM2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset for PM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other data\n",
    "X = weather_dataset [:-shift]\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.67, shuffle=False)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Data scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.187569\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.116009\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.084389\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.081148\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.079729\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.110599\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.089639\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.073596\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.061132\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.052825\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.053468\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.050684\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.047410\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.045793\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.044788\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.123025\n",
      ">> Epoch 1 finished \tANN training loss 0.120624\n",
      ">> Epoch 2 finished \tANN training loss 0.118832\n",
      ">> Epoch 3 finished \tANN training loss 0.117842\n",
      ">> Epoch 4 finished \tANN training loss 0.117348\n",
      ">> Epoch 5 finished \tANN training loss 0.116719\n",
      ">> Epoch 6 finished \tANN training loss 0.116331\n",
      ">> Epoch 7 finished \tANN training loss 0.115396\n",
      ">> Epoch 8 finished \tANN training loss 0.114984\n",
      ">> Epoch 9 finished \tANN training loss 0.114927\n",
      ">> Epoch 10 finished \tANN training loss 0.113697\n",
      ">> Epoch 11 finished \tANN training loss 0.112878\n",
      ">> Epoch 12 finished \tANN training loss 0.112037\n",
      ">> Epoch 13 finished \tANN training loss 0.111123\n",
      ">> Epoch 14 finished \tANN training loss 0.110187\n",
      ">> Epoch 15 finished \tANN training loss 0.109110\n",
      ">> Epoch 16 finished \tANN training loss 0.107783\n",
      ">> Epoch 17 finished \tANN training loss 0.106706\n",
      ">> Epoch 18 finished \tANN training loss 0.105272\n",
      ">> Epoch 19 finished \tANN training loss 0.104153\n",
      ">> Epoch 20 finished \tANN training loss 0.104464\n",
      ">> Epoch 21 finished \tANN training loss 0.102679\n",
      ">> Epoch 22 finished \tANN training loss 0.102625\n",
      ">> Epoch 23 finished \tANN training loss 0.100881\n",
      ">> Epoch 24 finished \tANN training loss 0.101307\n",
      ">> Epoch 25 finished \tANN training loss 0.100570\n",
      ">> Epoch 26 finished \tANN training loss 0.100548\n",
      ">> Epoch 27 finished \tANN training loss 0.099588\n",
      ">> Epoch 28 finished \tANN training loss 0.098627\n",
      ">> Epoch 29 finished \tANN training loss 0.098862\n",
      ">> Epoch 30 finished \tANN training loss 0.098179\n",
      ">> Epoch 31 finished \tANN training loss 0.097592\n",
      ">> Epoch 32 finished \tANN training loss 0.097492\n",
      ">> Epoch 33 finished \tANN training loss 0.098322\n",
      ">> Epoch 34 finished \tANN training loss 0.097037\n",
      ">> Epoch 35 finished \tANN training loss 0.096616\n",
      ">> Epoch 36 finished \tANN training loss 0.096680\n",
      ">> Epoch 37 finished \tANN training loss 0.098536\n",
      ">> Epoch 38 finished \tANN training loss 0.097623\n",
      ">> Epoch 39 finished \tANN training loss 0.095891\n",
      ">> Epoch 40 finished \tANN training loss 0.096676\n",
      ">> Epoch 41 finished \tANN training loss 0.095830\n",
      ">> Epoch 42 finished \tANN training loss 0.096967\n",
      ">> Epoch 43 finished \tANN training loss 0.095461\n",
      ">> Epoch 44 finished \tANN training loss 0.095372\n",
      ">> Epoch 45 finished \tANN training loss 0.095822\n",
      ">> Epoch 46 finished \tANN training loss 0.096235\n",
      ">> Epoch 47 finished \tANN training loss 0.094962\n",
      ">> Epoch 48 finished \tANN training loss 0.095150\n",
      ">> Epoch 49 finished \tANN training loss 0.095651\n",
      ">> Epoch 50 finished \tANN training loss 0.094563\n",
      ">> Epoch 51 finished \tANN training loss 0.094509\n",
      ">> Epoch 52 finished \tANN training loss 0.094458\n",
      ">> Epoch 53 finished \tANN training loss 0.094280\n",
      ">> Epoch 54 finished \tANN training loss 0.094257\n",
      ">> Epoch 55 finished \tANN training loss 0.094173\n",
      ">> Epoch 56 finished \tANN training loss 0.094453\n",
      ">> Epoch 57 finished \tANN training loss 0.095266\n",
      ">> Epoch 58 finished \tANN training loss 0.095204\n",
      ">> Epoch 59 finished \tANN training loss 0.093761\n",
      ">> Epoch 60 finished \tANN training loss 0.093725\n",
      ">> Epoch 61 finished \tANN training loss 0.094594\n",
      ">> Epoch 62 finished \tANN training loss 0.094144\n",
      ">> Epoch 63 finished \tANN training loss 0.093622\n",
      ">> Epoch 64 finished \tANN training loss 0.093760\n",
      ">> Epoch 65 finished \tANN training loss 0.093420\n",
      ">> Epoch 66 finished \tANN training loss 0.094205\n",
      ">> Epoch 67 finished \tANN training loss 0.093316\n",
      ">> Epoch 68 finished \tANN training loss 0.095145\n",
      ">> Epoch 69 finished \tANN training loss 0.093740\n",
      ">> Epoch 70 finished \tANN training loss 0.093018\n",
      ">> Epoch 71 finished \tANN training loss 0.092791\n",
      ">> Epoch 72 finished \tANN training loss 0.092823\n",
      ">> Epoch 73 finished \tANN training loss 0.093879\n",
      ">> Epoch 74 finished \tANN training loss 0.092623\n",
      ">> Epoch 75 finished \tANN training loss 0.095727\n",
      ">> Epoch 76 finished \tANN training loss 0.093526\n",
      ">> Epoch 77 finished \tANN training loss 0.092683\n",
      ">> Epoch 78 finished \tANN training loss 0.093181\n",
      ">> Epoch 79 finished \tANN training loss 0.092437\n",
      ">> Epoch 80 finished \tANN training loss 0.093132\n",
      ">> Epoch 81 finished \tANN training loss 0.093527\n",
      ">> Epoch 82 finished \tANN training loss 0.093230\n",
      ">> Epoch 83 finished \tANN training loss 0.094450\n",
      ">> Epoch 84 finished \tANN training loss 0.092132\n",
      ">> Epoch 85 finished \tANN training loss 0.092894\n",
      ">> Epoch 86 finished \tANN training loss 0.092045\n",
      ">> Epoch 87 finished \tANN training loss 0.092202\n",
      ">> Epoch 88 finished \tANN training loss 0.092783\n",
      ">> Epoch 89 finished \tANN training loss 0.095148\n",
      ">> Epoch 90 finished \tANN training loss 0.092019\n",
      ">> Epoch 91 finished \tANN training loss 0.092835\n",
      ">> Epoch 92 finished \tANN training loss 0.092592\n",
      ">> Epoch 93 finished \tANN training loss 0.091734\n",
      ">> Epoch 94 finished \tANN training loss 0.091930\n",
      ">> Epoch 95 finished \tANN training loss 0.091781\n",
      ">> Epoch 96 finished \tANN training loss 0.091649\n",
      ">> Epoch 97 finished \tANN training loss 0.092022\n",
      ">> Epoch 98 finished \tANN training loss 0.092293\n",
      ">> Epoch 99 finished \tANN training loss 0.091683\n",
      ">> Epoch 100 finished \tANN training loss 0.091921\n",
      ">> Epoch 101 finished \tANN training loss 0.092451\n",
      ">> Epoch 102 finished \tANN training loss 0.093156\n",
      ">> Epoch 103 finished \tANN training loss 0.091774\n",
      ">> Epoch 104 finished \tANN training loss 0.091351\n",
      ">> Epoch 105 finished \tANN training loss 0.091419\n",
      ">> Epoch 106 finished \tANN training loss 0.091265\n",
      ">> Epoch 107 finished \tANN training loss 0.091359\n",
      ">> Epoch 108 finished \tANN training loss 0.091403\n",
      ">> Epoch 109 finished \tANN training loss 0.091452\n",
      ">> Epoch 110 finished \tANN training loss 0.091254\n",
      ">> Epoch 111 finished \tANN training loss 0.091126\n",
      ">> Epoch 112 finished \tANN training loss 0.091871\n",
      ">> Epoch 113 finished \tANN training loss 0.091135\n",
      ">> Epoch 114 finished \tANN training loss 0.093533\n",
      ">> Epoch 115 finished \tANN training loss 0.091262\n",
      ">> Epoch 116 finished \tANN training loss 0.091107\n",
      ">> Epoch 117 finished \tANN training loss 0.091634\n",
      ">> Epoch 118 finished \tANN training loss 0.091558\n",
      ">> Epoch 119 finished \tANN training loss 0.091102\n",
      ">> Epoch 120 finished \tANN training loss 0.091145\n",
      ">> Epoch 121 finished \tANN training loss 0.091261\n",
      ">> Epoch 122 finished \tANN training loss 0.092305\n",
      ">> Epoch 123 finished \tANN training loss 0.091463\n",
      ">> Epoch 124 finished \tANN training loss 0.091092\n",
      ">> Epoch 125 finished \tANN training loss 0.090748\n",
      ">> Epoch 126 finished \tANN training loss 0.091046\n",
      ">> Epoch 127 finished \tANN training loss 0.091523\n",
      ">> Epoch 128 finished \tANN training loss 0.091329\n",
      ">> Epoch 129 finished \tANN training loss 0.090908\n",
      ">> Epoch 130 finished \tANN training loss 0.092865\n",
      ">> Epoch 131 finished \tANN training loss 0.090635\n",
      ">> Epoch 132 finished \tANN training loss 0.090592\n",
      ">> Epoch 133 finished \tANN training loss 0.093431\n",
      ">> Epoch 134 finished \tANN training loss 0.090780\n",
      ">> Epoch 135 finished \tANN training loss 0.092391\n",
      ">> Epoch 136 finished \tANN training loss 0.091379\n",
      ">> Epoch 137 finished \tANN training loss 0.091534\n",
      ">> Epoch 138 finished \tANN training loss 0.090849\n",
      ">> Epoch 139 finished \tANN training loss 0.091378\n",
      ">> Epoch 140 finished \tANN training loss 0.090804\n",
      ">> Epoch 141 finished \tANN training loss 0.090886\n",
      ">> Epoch 142 finished \tANN training loss 0.091589\n",
      ">> Epoch 143 finished \tANN training loss 0.091618\n",
      ">> Epoch 144 finished \tANN training loss 0.090466\n",
      ">> Epoch 145 finished \tANN training loss 0.090786\n",
      ">> Epoch 146 finished \tANN training loss 0.090654\n",
      ">> Epoch 147 finished \tANN training loss 0.090296\n",
      ">> Epoch 148 finished \tANN training loss 0.091102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 149 finished \tANN training loss 0.090450\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNRegression(batch_size=16, dropout_p=0, l2_regularization=1.0,\n",
       "            learning_rate=0.01, n_iter_backprop=150, verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm2 = SupervisedDBNRegression(hidden_layers_structure=HIDDEN_LAYER_STRUCT,\n",
    "                                    learning_rate_rbm=RBM_LEARNING_RATE,\n",
    "                                    learning_rate=DBN_LEARNING_RATE,\n",
    "                                    n_epochs_rbm=RBM_EPOCHS,\n",
    "                                    n_iter_backprop=DBN_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    activation_function=ACTIVE_FUNC)\n",
    "pm2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "R-squared: -0.089768\n",
      "MSE: 0.087244\n",
      "MAE: 0.231066\n"
     ]
    }
   ],
   "source": [
    "X_test = min_max_scaler.transform(X_test)\n",
    "Y_pred = pm2.predict(X_test)\n",
    "print('Done.\\nR-squared: %f\\nMSE: %f' % (r2_score(Y_test, Y_pred), mean_squared_error(Y_test, Y_pred)))\n",
    "print('MAE: %f' %(mean_absolute_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling Results\n",
    "temp = []\n",
    "for i in range(len(Y_pred)):\n",
    "    temp.append(Y_pred[i][0])\n",
    "d = {'Predicted': temp, 'Actual': Y_test}\n",
    "pm2_results = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Results into csv file\n",
    "pm2_results.to_csv(\"output/pm2_output_\" + ROAD + \"_\" + YEAR + EXT, encoding='utf-8', index=False)\n",
    "pm2.save('models/pm2_' + ROAD + '_' + YEAR +'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Fusion Center\n",
    "### Preparing Training Dataset for Fusion Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm1_results = pd.read_csv(\"output/pm1_output_\" + ROAD + \"_\" + YEAR + EXT, skipinitialspace=True)\n",
    "pm2_results = pd.read_csv(\"output/pm2_output_\" + ROAD + \"_\" + YEAR + EXT, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'PM1-Output': pm1_results.Predicted, 'PM2-Output': pm2_results.Predicted}\n",
    "fusion_dataset = pd.DataFrame(data=d)\n",
    "fusion_dataset = np.array(fusion_dataset)\n",
    "actual_dataset = pm1_results.Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-be Predicted variable\n",
    "Y = actual_dataset\n",
    "Y = Y.round(5)\n",
    "\n",
    "# Other data\n",
    "X = fusion_dataset\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.67, shuffle=False)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Data scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Fusion Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.080204\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.070645\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.062924\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.057098\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.051241\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.035660\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.035179\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.034521\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.033969\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.033293\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.003001\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.002977\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.002981\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.002967\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.002954\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.067234\n",
      ">> Epoch 1 finished \tANN training loss 0.067041\n",
      ">> Epoch 2 finished \tANN training loss 0.066808\n",
      ">> Epoch 3 finished \tANN training loss 0.066459\n",
      ">> Epoch 4 finished \tANN training loss 0.065937\n",
      ">> Epoch 5 finished \tANN training loss 0.065095\n",
      ">> Epoch 6 finished \tANN training loss 0.063676\n",
      ">> Epoch 7 finished \tANN training loss 0.061426\n",
      ">> Epoch 8 finished \tANN training loss 0.055462\n",
      ">> Epoch 9 finished \tANN training loss 0.043462\n",
      ">> Epoch 10 finished \tANN training loss 0.021507\n",
      ">> Epoch 11 finished \tANN training loss 0.006090\n",
      ">> Epoch 12 finished \tANN training loss 0.004079\n",
      ">> Epoch 13 finished \tANN training loss 0.003937\n",
      ">> Epoch 14 finished \tANN training loss 0.003990\n",
      ">> Epoch 15 finished \tANN training loss 0.003868\n",
      ">> Epoch 16 finished \tANN training loss 0.003890\n",
      ">> Epoch 17 finished \tANN training loss 0.003850\n",
      ">> Epoch 18 finished \tANN training loss 0.003899\n",
      ">> Epoch 19 finished \tANN training loss 0.003847\n",
      ">> Epoch 20 finished \tANN training loss 0.003842\n",
      ">> Epoch 21 finished \tANN training loss 0.003879\n",
      ">> Epoch 22 finished \tANN training loss 0.003840\n",
      ">> Epoch 23 finished \tANN training loss 0.003842\n",
      ">> Epoch 24 finished \tANN training loss 0.003861\n",
      ">> Epoch 25 finished \tANN training loss 0.003841\n",
      ">> Epoch 26 finished \tANN training loss 0.003866\n",
      ">> Epoch 27 finished \tANN training loss 0.003841\n",
      ">> Epoch 28 finished \tANN training loss 0.003839\n",
      ">> Epoch 29 finished \tANN training loss 0.003843\n",
      ">> Epoch 30 finished \tANN training loss 0.003840\n",
      ">> Epoch 31 finished \tANN training loss 0.003840\n",
      ">> Epoch 32 finished \tANN training loss 0.003843\n",
      ">> Epoch 33 finished \tANN training loss 0.003846\n",
      ">> Epoch 34 finished \tANN training loss 0.003845\n",
      ">> Epoch 35 finished \tANN training loss 0.003854\n",
      ">> Epoch 36 finished \tANN training loss 0.003844\n",
      ">> Epoch 37 finished \tANN training loss 0.003838\n",
      ">> Epoch 38 finished \tANN training loss 0.003841\n",
      ">> Epoch 39 finished \tANN training loss 0.003837\n",
      ">> Epoch 40 finished \tANN training loss 0.003843\n",
      ">> Epoch 41 finished \tANN training loss 0.003840\n",
      ">> Epoch 42 finished \tANN training loss 0.003957\n",
      ">> Epoch 43 finished \tANN training loss 0.003840\n",
      ">> Epoch 44 finished \tANN training loss 0.003842\n",
      ">> Epoch 45 finished \tANN training loss 0.003841\n",
      ">> Epoch 46 finished \tANN training loss 0.003844\n",
      ">> Epoch 47 finished \tANN training loss 0.003848\n",
      ">> Epoch 48 finished \tANN training loss 0.003840\n",
      ">> Epoch 49 finished \tANN training loss 0.003839\n",
      ">> Epoch 50 finished \tANN training loss 0.003837\n",
      ">> Epoch 51 finished \tANN training loss 0.003858\n",
      ">> Epoch 52 finished \tANN training loss 0.003839\n",
      ">> Epoch 53 finished \tANN training loss 0.003839\n",
      ">> Epoch 54 finished \tANN training loss 0.003839\n",
      ">> Epoch 55 finished \tANN training loss 0.003842\n",
      ">> Epoch 56 finished \tANN training loss 0.003842\n",
      ">> Epoch 57 finished \tANN training loss 0.003836\n",
      ">> Epoch 58 finished \tANN training loss 0.003836\n",
      ">> Epoch 59 finished \tANN training loss 0.003868\n",
      ">> Epoch 60 finished \tANN training loss 0.003856\n",
      ">> Epoch 61 finished \tANN training loss 0.003846\n",
      ">> Epoch 62 finished \tANN training loss 0.003843\n",
      ">> Epoch 63 finished \tANN training loss 0.003847\n",
      ">> Epoch 64 finished \tANN training loss 0.003837\n",
      ">> Epoch 65 finished \tANN training loss 0.003839\n",
      ">> Epoch 66 finished \tANN training loss 0.003838\n",
      ">> Epoch 67 finished \tANN training loss 0.004146\n",
      ">> Epoch 68 finished \tANN training loss 0.003835\n",
      ">> Epoch 69 finished \tANN training loss 0.003852\n",
      ">> Epoch 70 finished \tANN training loss 0.003843\n",
      ">> Epoch 71 finished \tANN training loss 0.003838\n",
      ">> Epoch 72 finished \tANN training loss 0.003836\n",
      ">> Epoch 73 finished \tANN training loss 0.003889\n",
      ">> Epoch 74 finished \tANN training loss 0.003869\n",
      ">> Epoch 75 finished \tANN training loss 0.003835\n",
      ">> Epoch 76 finished \tANN training loss 0.003849\n",
      ">> Epoch 77 finished \tANN training loss 0.003839\n",
      ">> Epoch 78 finished \tANN training loss 0.003835\n",
      ">> Epoch 79 finished \tANN training loss 0.003841\n",
      ">> Epoch 80 finished \tANN training loss 0.003865\n",
      ">> Epoch 81 finished \tANN training loss 0.003854\n",
      ">> Epoch 82 finished \tANN training loss 0.003836\n",
      ">> Epoch 83 finished \tANN training loss 0.003835\n",
      ">> Epoch 84 finished \tANN training loss 0.003844\n",
      ">> Epoch 85 finished \tANN training loss 0.003834\n",
      ">> Epoch 86 finished \tANN training loss 0.003859\n",
      ">> Epoch 87 finished \tANN training loss 0.003836\n",
      ">> Epoch 88 finished \tANN training loss 0.003841\n",
      ">> Epoch 89 finished \tANN training loss 0.003842\n",
      ">> Epoch 90 finished \tANN training loss 0.003834\n",
      ">> Epoch 91 finished \tANN training loss 0.003840\n",
      ">> Epoch 92 finished \tANN training loss 0.003841\n",
      ">> Epoch 93 finished \tANN training loss 0.003849\n",
      ">> Epoch 94 finished \tANN training loss 0.003858\n",
      ">> Epoch 95 finished \tANN training loss 0.003835\n",
      ">> Epoch 96 finished \tANN training loss 0.003840\n",
      ">> Epoch 97 finished \tANN training loss 0.003835\n",
      ">> Epoch 98 finished \tANN training loss 0.003835\n",
      ">> Epoch 99 finished \tANN training loss 0.003835\n",
      ">> Epoch 100 finished \tANN training loss 0.003835\n",
      ">> Epoch 101 finished \tANN training loss 0.003991\n",
      ">> Epoch 102 finished \tANN training loss 0.003845\n",
      ">> Epoch 103 finished \tANN training loss 0.003834\n",
      ">> Epoch 104 finished \tANN training loss 0.003877\n",
      ">> Epoch 105 finished \tANN training loss 0.003840\n",
      ">> Epoch 106 finished \tANN training loss 0.003835\n",
      ">> Epoch 107 finished \tANN training loss 0.003835\n",
      ">> Epoch 108 finished \tANN training loss 0.003837\n",
      ">> Epoch 109 finished \tANN training loss 0.003835\n",
      ">> Epoch 110 finished \tANN training loss 0.003839\n",
      ">> Epoch 111 finished \tANN training loss 0.003858\n",
      ">> Epoch 112 finished \tANN training loss 0.003841\n",
      ">> Epoch 113 finished \tANN training loss 0.003876\n",
      ">> Epoch 114 finished \tANN training loss 0.003856\n",
      ">> Epoch 115 finished \tANN training loss 0.003835\n",
      ">> Epoch 116 finished \tANN training loss 0.003840\n",
      ">> Epoch 117 finished \tANN training loss 0.003841\n",
      ">> Epoch 118 finished \tANN training loss 0.003835\n",
      ">> Epoch 119 finished \tANN training loss 0.003838\n",
      ">> Epoch 120 finished \tANN training loss 0.003835\n",
      ">> Epoch 121 finished \tANN training loss 0.003845\n",
      ">> Epoch 122 finished \tANN training loss 0.003835\n",
      ">> Epoch 123 finished \tANN training loss 0.003845\n",
      ">> Epoch 124 finished \tANN training loss 0.003835\n",
      ">> Epoch 125 finished \tANN training loss 0.003849\n",
      ">> Epoch 126 finished \tANN training loss 0.003837\n",
      ">> Epoch 127 finished \tANN training loss 0.003852\n",
      ">> Epoch 128 finished \tANN training loss 0.003836\n",
      ">> Epoch 129 finished \tANN training loss 0.003873\n",
      ">> Epoch 130 finished \tANN training loss 0.003843\n",
      ">> Epoch 131 finished \tANN training loss 0.003834\n",
      ">> Epoch 132 finished \tANN training loss 0.003834\n",
      ">> Epoch 133 finished \tANN training loss 0.003834\n",
      ">> Epoch 134 finished \tANN training loss 0.003834\n",
      ">> Epoch 135 finished \tANN training loss 0.003879\n",
      ">> Epoch 136 finished \tANN training loss 0.003834\n",
      ">> Epoch 137 finished \tANN training loss 0.003862\n",
      ">> Epoch 138 finished \tANN training loss 0.003835\n",
      ">> Epoch 139 finished \tANN training loss 0.003836\n",
      ">> Epoch 140 finished \tANN training loss 0.003834\n",
      ">> Epoch 141 finished \tANN training loss 0.003842\n",
      ">> Epoch 142 finished \tANN training loss 0.003834\n",
      ">> Epoch 143 finished \tANN training loss 0.003841\n",
      ">> Epoch 144 finished \tANN training loss 0.003834\n",
      ">> Epoch 145 finished \tANN training loss 0.003835\n",
      ">> Epoch 146 finished \tANN training loss 0.003834\n",
      ">> Epoch 147 finished \tANN training loss 0.003833\n",
      ">> Epoch 148 finished \tANN training loss 0.003886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 149 finished \tANN training loss 0.003834\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNRegression(batch_size=16, dropout_p=0, l2_regularization=1.0,\n",
       "            learning_rate=0.01, n_iter_backprop=150, verbose=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "fc = SupervisedDBNRegression(hidden_layers_structure=HIDDEN_LAYER_STRUCT,\n",
    "                                    learning_rate_rbm=RBM_LEARNING_RATE,\n",
    "                                    learning_rate=DBN_LEARNING_RATE,\n",
    "                                    n_epochs_rbm=RBM_EPOCHS,\n",
    "                                    n_iter_backprop=DBN_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    activation_function=ACTIVE_FUNC)\n",
    "fc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Fusion Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "R-squared: 0.917420\n",
      "MSE: 0.007064\n",
      "MAE: 0.021589\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "Y_pred = fc.predict(X_test)\n",
    "print('Done.\\nR-squared: %f\\nMSE: %f' % (r2_score(Y_test, Y_pred), mean_squared_error(Y_test, Y_pred)))\n",
    "print('MAE: %f' %(mean_absolute_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(Y_pred)):\n",
    "    temp.append(Y_pred[i][0])\n",
    "d = {'Predicted': temp, 'Actual': Y_test}\n",
    "fc_results = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results.to_csv(\"output/fc_output_\" + ROAD  + \"_\" + YEAR + EXT, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "fc.save('models/fc_' + ROAD + '_' + YEAR + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
