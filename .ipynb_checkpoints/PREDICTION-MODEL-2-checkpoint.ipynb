{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.regression import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from dbn.tensorflow import SupervisedDBNRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM_EPOCHS = 5\n",
    "DBN_EPOCHS = 150\n",
    "RBM_LEARNING_RATE = 0.01\n",
    "DBN_LEARNING_RATE = 0.01\n",
    "HIDDEN_LAYER_STRUCT = [20, 50, 100]\n",
    "ACTIVE_FUNC = 'relu'\n",
    "BATCH_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Directory, Road, and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "ROAD = \"Vicente Cruz\"\n",
    "YEAR = \"2015\"\n",
    "EXT = \".csv\"\n",
    "DATASET_DIVISION = \"seasonWet\"\n",
    "DIR = \"../../../datasets/Thesis Datasets/\"\n",
    "\n",
    "'''''''Training dataset'''''''\n",
    "WP = True\n",
    "weatherDT = \"recon_weather\" #orig_weather   recon_weather\n",
    "featureEngineering = \" \" #Rolling   Expanding   Rolling and Expanding \n",
    "featuresNeeded = ['tempC', 'windspeedMiles', 'precipMM', 'visibility', #All Features\n",
    "                'humidity', 'pressure', 'cloudcover', 'heatIndexC',\n",
    "                'dewPointC', 'windChillC', 'windGustMiles', 'feelsLikeC']\n",
    "# featuresNeeded = ['tempC', 'windspeedMiles', 'humidity', 'heatIndexC', #All corr > 0.1\n",
    "#                 'dewPointC','windChillC', 'feelsLikeC']\n",
    "featuresNeeded = ['heatIndexC', 'windspeedMiles', 'dewPointC']\n",
    "ROLLING_WINDOW = 3\n",
    "EXPANDING_WINDOW = 12\n",
    "RECON_SHIFT = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM_EPOCHS = 5\n",
    "DBN_EPOCHS = 150\n",
    "RBM_LEARNING_RATE = 0.01\n",
    "DBN_LEARNING_RATE = 0.01\n",
    "HIDDEN_LAYER_STRUCT = [20, 50, 100]\n",
    "ACTIVE_FUNC = 'relu'\n",
    "BATCH_SIZE = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructDT(df, typeDataset='traffic', trafficFeatureNeeded=[]):\n",
    "    result_df = df.copy()\n",
    "\n",
    "    # Converting the index as date\n",
    "    result_df.index = pd.to_datetime(result_df.index, format='%d/%m/%Y %H:%M')\n",
    "#     result_df['year'] = result_df.index.year\n",
    "    result_df['month'] = result_df.index.month\n",
    "#     result_df['day'] = result_df.index.day\n",
    "    result_df['hour'] = result_df.index.hour\n",
    "    result_df['min'] = result_df.index.minute    \n",
    "    result_df['dayOfWeek'] = result_df.index.dayofweek\n",
    "    \n",
    "    if typeDataset == 'traffic':\n",
    "        for f in trafficFeatureNeeded:\n",
    "            result_df[f + '_' + str(RECON_SHIFT)] = result_df[f].shift(RECON_SHIFT)    \n",
    "            result_df[f + '_' + str(RECON_SHIFT)] = result_df[f].shift(RECON_SHIFT)\n",
    "            \n",
    "    result_df = result_df.iloc[96:, :]\n",
    "    \n",
    "    for f in range(len(result_df.columns)):\n",
    "        result_df[result_df.columns[f]] = normalize(result_df[result_df.columns[f]])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def shiftDTForReconstructed(df):\n",
    "    return df.iloc[shift:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeededFeatures(columns, arrFeaturesNeed=[], featureEngineering=\"Original\"):\n",
    "    to_remove = []\n",
    "    if len(arrFeaturesNeed) == 0: #all features aren't needed\n",
    "        return []\n",
    "    else:\n",
    "        if featureEngineering == \"Original\":\n",
    "            compareTo = \" \"\n",
    "        elif featureEngineering == \"Rolling\" or featureEngineering == \"Expanding\":\n",
    "            compareTo = \"_\"\n",
    "            \n",
    "        for f in arrFeaturesNeed:\n",
    "            for c in range(0, len(columns)):\n",
    "                if (len(columns[c].split(compareTo)) <= 1 and featureEngineering != \"Original\"):\n",
    "                    to_remove.append(c)\n",
    "                if f not in columns[c].split(compareTo)[0] and columns[c].split(compareTo)[0] not in arrFeaturesNeed:\n",
    "                    to_remove.append(c)\n",
    "                        \n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    y = pd.to_numeric(data)\n",
    "    y = np.array(y.reshape(-1, 1))\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    y = scaler.fit_transform(y)\n",
    "    y = y.reshape(1, -1)[0]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Preparing Traffic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Original Traffic (wo new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAFFIC_WINDOWSIZE = 3\n",
    "TRAFFIC_DIR = DIR + \"mmda/\"\n",
    "TRAFFIC_FILENAME = \"mmda_\" + ROAD + \"_\" + YEAR + \"_\" + DATASET_DIVISION\n",
    "orig_traffic = pd.read_csv(TRAFFIC_DIR + TRAFFIC_FILENAME + EXT, skipinitialspace=True)\n",
    "orig_traffic = orig_traffic.fillna(0)\n",
    "\n",
    "#Converting index to date and time, and removing 'dt' column\n",
    "orig_traffic.index = pd.to_datetime(orig_traffic.dt, format='%d/%m/%Y %H:%M')\n",
    "cols_to_remove = [0]\n",
    "cols_to_remove = getNeededFeatures(orig_traffic.columns, [\"statusN\"])\n",
    "orig_traffic.drop(orig_traffic.columns[[cols_to_remove]], axis=1, inplace=True)\n",
    "orig_traffic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Original Weather (wo new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_DIR = DIR + \"wwo/\"\n",
    "WEATHER_FILENAME = \"wwo_\" + ROAD + \"_\" + YEAR + \"_\" + DATASET_DIVISION\n",
    "\n",
    "orig_weather = pd.read_csv(WEATHER_DIR + WEATHER_FILENAME + EXT, skipinitialspace=True)\n",
    "\n",
    "cols_to_remove = [0]\n",
    "cols_to_remove += getNeededFeatures(orig_weather.columns, arrFeaturesNeed=featuresNeeded)\n",
    "\n",
    "orig_weather.index = pd.to_datetime(orig_weather.dt, format='%d/%m/%Y %H:%M')\n",
    "orig_weather.drop(orig_weather.columns[[cols_to_remove]], axis=1, inplace=True)\n",
    "orig_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Weather with Rolling features (with only needed features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_DIR = DIR + \"wwo/Rolling/\" + DATASET_DIVISION + \"/\"\n",
    "WEATHER_FILENAME = \"eng_win\" + str(ROLLING_WINDOW) + \"_wwo_\" + ROAD + \"_\" + YEAR + \"_\" + DATASET_DIVISION\n",
    "rolling_weather = pd.read_csv(WEATHER_DIR + WEATHER_FILENAME + EXT, skipinitialspace=True)\n",
    "\n",
    "cols_to_remove = []\n",
    "cols_to_remove += getNeededFeatures(rolling_weather.columns, orig_weather.columns, featureEngineering=\"Rolling\")\n",
    "rolling_weather.index = pd.to_datetime(rolling_weather.dt, format='%Y-%m-%d %H:%M')\n",
    "\n",
    "rolling_weather.drop(rolling_weather.columns[[cols_to_remove]], axis=1, inplace=True)\n",
    "\n",
    "for f in range(len(rolling_weather.columns)):\n",
    "        rolling_weather[rolling_weather.columns[f]] = normalize(rolling_weather[rolling_weather.columns[f]])\n",
    "\n",
    "rolling_weather.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Weather with Expanding features (with only needed features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_DIR = DIR + \"wwo/Expanding/\" + DATASET_DIVISION + \"/\"\n",
    "WEATHER_FILENAME = \"eng_win\" + str(EXPANDING_WINDOW) + \"_wwo_\" + ROAD + \"_\" + YEAR + \"_\" + DATASET_DIVISION\n",
    "expanding_weather = pd.read_csv(WEATHER_DIR + WEATHER_FILENAME + EXT, skipinitialspace=True)\n",
    "\n",
    "cols_to_remove = []\n",
    "cols_to_remove += getNeededFeatures(expanding_weather.columns, orig_weather.columns, featureEngineering=\"Rolling\")\n",
    "     \n",
    "expanding_weather.index = pd.to_datetime(expanding_weather.dt, format='%d/%m/%Y %H:%M')\n",
    "\n",
    "expanding_weather.drop(expanding_weather.columns[[cols_to_remove]], axis=1, inplace=True)\n",
    "\n",
    "for f in range(len(expanding_weather.columns)):\n",
    "        expanding_weather[expanding_weather.columns[f]] = normalize(expanding_weather[expanding_weather.columns[f]])\n",
    "\n",
    "expanding_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing Weather Input for recon_weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_weather = reconstructDT(orig_weather, 'weather', ['statusN'])\n",
    "recon_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''' Do not touch below '''''''\n",
    "\n",
    "if weatherDT == \"orig_weather\":\n",
    "    print(\"Adding orig_weather\")\n",
    "    arrDT = [orig_traffic, orig_weather]\n",
    "    \n",
    "elif weatherDT == \"recon_weather\":\n",
    "    print(\"Adding recon_weather\")\n",
    "    arrDT = [orig_traffic.iloc[96:, :], recon_weather]\n",
    "    \n",
    "if featureEngineering == \"Rolling\":\n",
    "    print(\"Adding Rolling\")\n",
    "    startIndex = np.absolute(len(arrDT[0])-len(rolling_weather))\n",
    "    temp = rolling_weather.iloc[startIndex:, :]\n",
    "    arrDT.append(temp)\n",
    "    \n",
    "elif featureEngineering == \"Expanding\":\n",
    "    print(\"Adding Expanding\")\n",
    "    startIndex = np.absolute(len(arrDT[0])-len(expanding_weather))\n",
    "    temp = expanding_weather.iloc[startIndex:, :]\n",
    "    arrDT.append(temp)\n",
    "    \n",
    "elif featureEngineering == \"Rolling and Expanding\":\n",
    "    print(\"Adding Rolling and Expanding\")\n",
    "    #Rolling\n",
    "    startIndex = np.absolute(len(arrDT[0])-len(rolling_weather))\n",
    "    temp = rolling_weather.iloc[startIndex:, :]\n",
    "    arrDT.append(temp)\n",
    "    #Expanding\n",
    "    startIndex = np.absolute(len(arrDT[0])-len(expanding_weather))\n",
    "    temp = expanding_weather.iloc[startIndex:, :]\n",
    "    arrDT.append(temp)\n",
    "\n",
    "merged_dataset = pd.concat(arrDT, axis=1)\n",
    "merged_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Original (and Rolling and Expanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-be Predicted variable \n",
    "Y = merged_dataset.statusN\n",
    "Y = Y.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "X = merged_dataset\n",
    "X = X.drop(X.columns[[0]], axis=1)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.67, shuffle=False)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Data scaling\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "# X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "#Print training and testing data\n",
    "pd.concat([X, Y.to_frame()], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "regressor = SupervisedDBNRegression(hidden_layers_structure=HIDDEN_LAYER_STRUCT,\n",
    "                                    learning_rate_rbm=RBM_LEARNING_RATE,\n",
    "                                    learning_rate=DBN_LEARNING_RATE,\n",
    "                                    n_epochs_rbm=RBM_EPOCHS,\n",
    "                                    n_iter_backprop=DBN_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    activation_function=ACTIVE_FUNC)\n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check RBM Loss Errors:\n",
    "rbm_error = regressor.unsupervised_dbn.rbm_layers[0].rbm_loss_error\n",
    "#To check DBN Loss Errors\n",
    "dbn_error = regressor.dbn_loss_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# X_test = min_max_scaler.transform(X_test)\n",
    "# Y_pred = regressor.predict(X_test)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_test = min_max_scaler.fit_transform(X_test)\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "r2score = r2_score(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "print('Done.\\nR-squared: %.3f\\nRMSE: %.3f \\nMAE: %.3f' % (r2score, rmse, mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "regressor.save('models/pm2_' + ROAD + '_' + YEAR + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Analysis below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing Predicted and Actual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndex = orig_weather.shape[0] - Y_pred.shape[0]\n",
    "dt = orig_weather.index[startIndex:,]\n",
    "temp = []\n",
    "for i in range(len(Y_pred)):\n",
    "    temp.append(Y_pred[i][0])\n",
    "d = {'Predicted': temp, 'Actual': Y_test, 'dt':dt}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output/pm2_\" + ROAD + \"_\" + YEAR + EXT, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize trend of loss of RBM and DBN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = df.Actual\n",
    "line2 = df.Predicted\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(line1, c='red', alpha=0.4)\n",
    "plt.plot(line2, c='blue', alpha=0.4)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Traffic Speed\")\n",
    "plt.yticks([0, 0.5, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = rbm_error\n",
    "line2 = dbn_error\n",
    "x = range(0, RBM_EPOCHS * len(HIDDEN_LAYER_STRUCT))\n",
    "plt.plot(range(0, RBM_EPOCHS * len(HIDDEN_LAYER_STRUCT)), line1, c='red')\n",
    "plt.xticks(x)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(DBN_EPOCHS), line2, c='blue')\n",
    "plt.xticks(x)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(0, RBM_EPOCHS * len(HIDDEN_LAYER_STRUCT)), line1, c='red')\n",
    "plt.plot(range(DBN_EPOCHS), line2, c='blue')\n",
    "plt.xticks(x)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
