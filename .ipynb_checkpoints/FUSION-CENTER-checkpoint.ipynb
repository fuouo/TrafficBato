{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.regression import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dbn.tensorflow import SupervisedDBNRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Road and Year for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "ROAD = \"Taft Ave.\"\n",
    "YEAR = \"2015\"\n",
    "EXT = \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM_EPOCHS = 5\n",
    "DBN_EPOCHS = 150\n",
    "RBM_LEARNING_RATE = 0.01\n",
    "DBN_LEARNING_RATE = 0.01\n",
    "HIDDEN_LAYER_STRUCT = [20, 50, 100]\n",
    "ACTIVE_FUNC = 'relu'\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing PM1 and PM2 Output dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_data = pd.read_csv(\"data/mmda/eng_win2_mmda_\" + ROAD + \"_\" + YEAR + EXT, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm1_results = pd.read_csv(\"output/pm1_output_\" + ROAD + \"_\" + YEAR + EXT, skipinitialspace=True)\n",
    "pm2_results = pd.read_csv(\"output/pm2_output_\" + ROAD + \"_\" + YEAR + EXT, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'PM1-Output': pm1_results.Predicted, 'PM2-Output': pm2_results.Predicted}\n",
    "fusion_dataset = pd.DataFrame(data=d)\n",
    "fusion_dataset = np.array(fusion_dataset)\n",
    "actual_dataset = pm1_results.Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-be Predicted variable\n",
    "Y = actual_dataset\n",
    "Y = Y.round(5)\n",
    "\n",
    "# Other data\n",
    "X = fusion_dataset\n",
    "\n",
    "# Splitting data\n",
    "X_train = X\n",
    "Y_train = Y\n",
    "X_test = X_train\n",
    "Y_test = Y_train\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Data scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Fusion Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.048787\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.044702\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.040769\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.036798\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.036308\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.017163\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.016946\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.016783\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.016863\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.016335\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.001140\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.001227\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.001123\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.001132\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.001182\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.035447\n",
      ">> Epoch 1 finished \tANN training loss 0.035206\n",
      ">> Epoch 2 finished \tANN training loss 0.034770\n",
      ">> Epoch 3 finished \tANN training loss 0.033682\n",
      ">> Epoch 4 finished \tANN training loss 0.031080\n",
      ">> Epoch 5 finished \tANN training loss 0.022698\n",
      ">> Epoch 6 finished \tANN training loss 0.004367\n",
      ">> Epoch 7 finished \tANN training loss 0.000296\n",
      ">> Epoch 8 finished \tANN training loss 0.000089\n",
      ">> Epoch 9 finished \tANN training loss 0.000035\n",
      ">> Epoch 10 finished \tANN training loss 0.000019\n",
      ">> Epoch 11 finished \tANN training loss 0.000014\n",
      ">> Epoch 12 finished \tANN training loss 0.000011\n",
      ">> Epoch 13 finished \tANN training loss 0.000010\n",
      ">> Epoch 14 finished \tANN training loss 0.000009\n",
      ">> Epoch 15 finished \tANN training loss 0.000009\n",
      ">> Epoch 16 finished \tANN training loss 0.000009\n",
      ">> Epoch 17 finished \tANN training loss 0.000008\n",
      ">> Epoch 18 finished \tANN training loss 0.000008\n",
      ">> Epoch 19 finished \tANN training loss 0.000008\n",
      ">> Epoch 20 finished \tANN training loss 0.000008\n",
      ">> Epoch 21 finished \tANN training loss 0.000008\n",
      ">> Epoch 22 finished \tANN training loss 0.000007\n",
      ">> Epoch 23 finished \tANN training loss 0.000007\n",
      ">> Epoch 24 finished \tANN training loss 0.000007\n",
      ">> Epoch 25 finished \tANN training loss 0.000007\n",
      ">> Epoch 26 finished \tANN training loss 0.000007\n",
      ">> Epoch 27 finished \tANN training loss 0.000007\n",
      ">> Epoch 28 finished \tANN training loss 0.000007\n",
      ">> Epoch 29 finished \tANN training loss 0.000007\n",
      ">> Epoch 30 finished \tANN training loss 0.000007\n",
      ">> Epoch 31 finished \tANN training loss 0.000007\n",
      ">> Epoch 32 finished \tANN training loss 0.000007\n",
      ">> Epoch 33 finished \tANN training loss 0.000007\n",
      ">> Epoch 34 finished \tANN training loss 0.000007\n",
      ">> Epoch 35 finished \tANN training loss 0.000007\n",
      ">> Epoch 36 finished \tANN training loss 0.000007\n",
      ">> Epoch 37 finished \tANN training loss 0.000007\n",
      ">> Epoch 38 finished \tANN training loss 0.000007\n",
      ">> Epoch 39 finished \tANN training loss 0.000007\n",
      ">> Epoch 40 finished \tANN training loss 0.000007\n",
      ">> Epoch 41 finished \tANN training loss 0.000007\n",
      ">> Epoch 42 finished \tANN training loss 0.000007\n",
      ">> Epoch 43 finished \tANN training loss 0.000007\n",
      ">> Epoch 44 finished \tANN training loss 0.000007\n",
      ">> Epoch 45 finished \tANN training loss 0.000007\n",
      ">> Epoch 46 finished \tANN training loss 0.000007\n",
      ">> Epoch 47 finished \tANN training loss 0.000007\n",
      ">> Epoch 48 finished \tANN training loss 0.000007\n",
      ">> Epoch 49 finished \tANN training loss 0.000007\n",
      ">> Epoch 50 finished \tANN training loss 0.000007\n",
      ">> Epoch 51 finished \tANN training loss 0.000007\n",
      ">> Epoch 52 finished \tANN training loss 0.000007\n",
      ">> Epoch 53 finished \tANN training loss 0.000007\n",
      ">> Epoch 54 finished \tANN training loss 0.000007\n",
      ">> Epoch 55 finished \tANN training loss 0.000007\n",
      ">> Epoch 56 finished \tANN training loss 0.000007\n",
      ">> Epoch 57 finished \tANN training loss 0.000007\n",
      ">> Epoch 58 finished \tANN training loss 0.000007\n",
      ">> Epoch 59 finished \tANN training loss 0.000007\n",
      ">> Epoch 60 finished \tANN training loss 0.000007\n",
      ">> Epoch 61 finished \tANN training loss 0.000007\n",
      ">> Epoch 62 finished \tANN training loss 0.000007\n",
      ">> Epoch 63 finished \tANN training loss 0.000007\n",
      ">> Epoch 64 finished \tANN training loss 0.000007\n",
      ">> Epoch 65 finished \tANN training loss 0.000007\n",
      ">> Epoch 66 finished \tANN training loss 0.000007\n",
      ">> Epoch 67 finished \tANN training loss 0.000007\n",
      ">> Epoch 68 finished \tANN training loss 0.000007\n",
      ">> Epoch 69 finished \tANN training loss 0.000007\n",
      ">> Epoch 70 finished \tANN training loss 0.000007\n",
      ">> Epoch 71 finished \tANN training loss 0.000007\n",
      ">> Epoch 72 finished \tANN training loss 0.000007\n",
      ">> Epoch 73 finished \tANN training loss 0.000007\n",
      ">> Epoch 74 finished \tANN training loss 0.000007\n",
      ">> Epoch 75 finished \tANN training loss 0.000007\n",
      ">> Epoch 76 finished \tANN training loss 0.000007\n",
      ">> Epoch 77 finished \tANN training loss 0.000007\n",
      ">> Epoch 78 finished \tANN training loss 0.000007\n",
      ">> Epoch 79 finished \tANN training loss 0.000007\n",
      ">> Epoch 80 finished \tANN training loss 0.000006\n",
      ">> Epoch 81 finished \tANN training loss 0.000007\n",
      ">> Epoch 82 finished \tANN training loss 0.000007\n",
      ">> Epoch 83 finished \tANN training loss 0.000006\n",
      ">> Epoch 84 finished \tANN training loss 0.000007\n",
      ">> Epoch 85 finished \tANN training loss 0.000006\n",
      ">> Epoch 86 finished \tANN training loss 0.000006\n",
      ">> Epoch 87 finished \tANN training loss 0.000006\n",
      ">> Epoch 88 finished \tANN training loss 0.000006\n",
      ">> Epoch 89 finished \tANN training loss 0.000006\n",
      ">> Epoch 90 finished \tANN training loss 0.000006\n",
      ">> Epoch 91 finished \tANN training loss 0.000006\n",
      ">> Epoch 92 finished \tANN training loss 0.000006\n",
      ">> Epoch 93 finished \tANN training loss 0.000006\n",
      ">> Epoch 94 finished \tANN training loss 0.000006\n",
      ">> Epoch 95 finished \tANN training loss 0.000007\n",
      ">> Epoch 96 finished \tANN training loss 0.000007\n",
      ">> Epoch 97 finished \tANN training loss 0.000006\n",
      ">> Epoch 98 finished \tANN training loss 0.000006\n",
      ">> Epoch 99 finished \tANN training loss 0.000006\n",
      ">> Epoch 100 finished \tANN training loss 0.000006\n",
      ">> Epoch 101 finished \tANN training loss 0.000006\n",
      ">> Epoch 102 finished \tANN training loss 0.000006\n",
      ">> Epoch 103 finished \tANN training loss 0.000006\n",
      ">> Epoch 104 finished \tANN training loss 0.000006\n",
      ">> Epoch 105 finished \tANN training loss 0.000006\n",
      ">> Epoch 106 finished \tANN training loss 0.000006\n",
      ">> Epoch 107 finished \tANN training loss 0.000006\n",
      ">> Epoch 108 finished \tANN training loss 0.000006\n",
      ">> Epoch 109 finished \tANN training loss 0.000006\n",
      ">> Epoch 110 finished \tANN training loss 0.000006\n",
      ">> Epoch 111 finished \tANN training loss 0.000007\n",
      ">> Epoch 112 finished \tANN training loss 0.000006\n",
      ">> Epoch 113 finished \tANN training loss 0.000006\n",
      ">> Epoch 114 finished \tANN training loss 0.000006\n",
      ">> Epoch 115 finished \tANN training loss 0.000006\n",
      ">> Epoch 116 finished \tANN training loss 0.000006\n",
      ">> Epoch 117 finished \tANN training loss 0.000006\n",
      ">> Epoch 118 finished \tANN training loss 0.000006\n",
      ">> Epoch 119 finished \tANN training loss 0.000006\n",
      ">> Epoch 120 finished \tANN training loss 0.000006\n",
      ">> Epoch 121 finished \tANN training loss 0.000006\n",
      ">> Epoch 122 finished \tANN training loss 0.000006\n",
      ">> Epoch 123 finished \tANN training loss 0.000006\n",
      ">> Epoch 124 finished \tANN training loss 0.000006\n",
      ">> Epoch 125 finished \tANN training loss 0.000006\n",
      ">> Epoch 126 finished \tANN training loss 0.000006\n",
      ">> Epoch 127 finished \tANN training loss 0.000006\n",
      ">> Epoch 128 finished \tANN training loss 0.000006\n",
      ">> Epoch 129 finished \tANN training loss 0.000006\n",
      ">> Epoch 130 finished \tANN training loss 0.000006\n",
      ">> Epoch 131 finished \tANN training loss 0.000006\n",
      ">> Epoch 132 finished \tANN training loss 0.000006\n",
      ">> Epoch 133 finished \tANN training loss 0.000006\n",
      ">> Epoch 134 finished \tANN training loss 0.000006\n",
      ">> Epoch 135 finished \tANN training loss 0.000006\n",
      ">> Epoch 136 finished \tANN training loss 0.000006\n",
      ">> Epoch 137 finished \tANN training loss 0.000006\n",
      ">> Epoch 138 finished \tANN training loss 0.000006\n",
      ">> Epoch 139 finished \tANN training loss 0.000006\n",
      ">> Epoch 140 finished \tANN training loss 0.000006\n",
      ">> Epoch 141 finished \tANN training loss 0.000006\n",
      ">> Epoch 142 finished \tANN training loss 0.000006\n",
      ">> Epoch 143 finished \tANN training loss 0.000006\n",
      ">> Epoch 144 finished \tANN training loss 0.000006\n",
      ">> Epoch 145 finished \tANN training loss 0.000006\n",
      ">> Epoch 146 finished \tANN training loss 0.000006\n",
      ">> Epoch 147 finished \tANN training loss 0.000006\n",
      ">> Epoch 148 finished \tANN training loss 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 149 finished \tANN training loss 0.000006\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNRegression(batch_size=16, dropout_p=0, l2_regularization=1.0,\n",
       "            learning_rate=0.01, n_iter_backprop=150, verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "fc = SupervisedDBNRegression(hidden_layers_structure=HIDDEN_LAYER_STRUCT,\n",
    "                                    learning_rate_rbm=RBM_LEARNING_RATE,\n",
    "                                    learning_rate=DBN_LEARNING_RATE,\n",
    "                                    n_epochs_rbm=RBM_EPOCHS,\n",
    "                                    n_iter_backprop=DBN_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    activation_function=ACTIVE_FUNC)\n",
    "fc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Fusion Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "R-squared: 0.999823\n",
      "MSE: 0.000006\n",
      "MAE: 0.001904\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "Y_pred = fc.predict(X_test)\n",
    "print('Done.\\nR-squared: %f\\nMSE: %f' % (r2_score(Y_test, Y_pred), mean_squared_error(Y_test, Y_pred)))\n",
    "print('MAE: %f' %(mean_absolute_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting predicted and actual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndex = traffic_data.shape[0] - fc_results.shape[0]\n",
    "dt = traffic_data.dt[startIndex:,]\n",
    "temp = []\n",
    "for i in range(len(Y_pred)):\n",
    "    temp.append(Y_pred[i][0])\n",
    "d = {'Predicted': temp, 'Actual': Y_test, 'dt':dt}\n",
    "fc_results = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11563</th>\n",
       "      <td>0.34164</td>\n",
       "      <td>0.342362</td>\n",
       "      <td>01/05/2015 10:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11564</th>\n",
       "      <td>0.33944</td>\n",
       "      <td>0.340204</td>\n",
       "      <td>01/05/2015 11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11565</th>\n",
       "      <td>0.33737</td>\n",
       "      <td>0.338181</td>\n",
       "      <td>01/05/2015 11:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>0.33543</td>\n",
       "      <td>0.336288</td>\n",
       "      <td>01/05/2015 11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11567</th>\n",
       "      <td>0.33362</td>\n",
       "      <td>0.334522</td>\n",
       "      <td>01/05/2015 11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11568</th>\n",
       "      <td>0.33193</td>\n",
       "      <td>0.332879</td>\n",
       "      <td>01/05/2015 12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11569</th>\n",
       "      <td>0.33037</td>\n",
       "      <td>0.331339</td>\n",
       "      <td>01/05/2015 12:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11570</th>\n",
       "      <td>0.32892</td>\n",
       "      <td>0.329909</td>\n",
       "      <td>01/05/2015 12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>0.32758</td>\n",
       "      <td>0.328574</td>\n",
       "      <td>01/05/2015 12:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11572</th>\n",
       "      <td>0.32636</td>\n",
       "      <td>0.327350</td>\n",
       "      <td>01/05/2015 13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11573</th>\n",
       "      <td>0.32524</td>\n",
       "      <td>0.326232</td>\n",
       "      <td>01/05/2015 13:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11574</th>\n",
       "      <td>0.32423</td>\n",
       "      <td>0.325224</td>\n",
       "      <td>01/05/2015 13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11575</th>\n",
       "      <td>0.32331</td>\n",
       "      <td>0.324319</td>\n",
       "      <td>01/05/2015 13:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11576</th>\n",
       "      <td>0.32250</td>\n",
       "      <td>0.323511</td>\n",
       "      <td>01/05/2015 14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>0.32178</td>\n",
       "      <td>0.322793</td>\n",
       "      <td>01/05/2015 14:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11578</th>\n",
       "      <td>0.32114</td>\n",
       "      <td>0.322170</td>\n",
       "      <td>01/05/2015 14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11579</th>\n",
       "      <td>0.32060</td>\n",
       "      <td>0.321632</td>\n",
       "      <td>01/05/2015 14:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11580</th>\n",
       "      <td>0.32014</td>\n",
       "      <td>0.321177</td>\n",
       "      <td>01/05/2015 15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11581</th>\n",
       "      <td>0.31976</td>\n",
       "      <td>0.320803</td>\n",
       "      <td>01/05/2015 15:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11582</th>\n",
       "      <td>0.31946</td>\n",
       "      <td>0.320505</td>\n",
       "      <td>01/05/2015 15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11583</th>\n",
       "      <td>0.31923</td>\n",
       "      <td>0.320280</td>\n",
       "      <td>01/05/2015 15:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11584</th>\n",
       "      <td>0.31907</td>\n",
       "      <td>0.320125</td>\n",
       "      <td>01/05/2015 16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>0.31898</td>\n",
       "      <td>0.320038</td>\n",
       "      <td>01/05/2015 16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>0.31895</td>\n",
       "      <td>0.320006</td>\n",
       "      <td>01/05/2015 16:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11587</th>\n",
       "      <td>0.31898</td>\n",
       "      <td>0.320035</td>\n",
       "      <td>01/05/2015 16:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11588</th>\n",
       "      <td>0.31907</td>\n",
       "      <td>0.320121</td>\n",
       "      <td>01/05/2015 17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11589</th>\n",
       "      <td>0.31921</td>\n",
       "      <td>0.320260</td>\n",
       "      <td>01/05/2015 17:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11590</th>\n",
       "      <td>0.31940</td>\n",
       "      <td>0.320424</td>\n",
       "      <td>01/05/2015 17:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11591</th>\n",
       "      <td>0.31964</td>\n",
       "      <td>0.320706</td>\n",
       "      <td>01/05/2015 17:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11592</th>\n",
       "      <td>0.31992</td>\n",
       "      <td>0.321043</td>\n",
       "      <td>01/05/2015 18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35010</th>\n",
       "      <td>0.38542</td>\n",
       "      <td>0.384911</td>\n",
       "      <td>31/12/2015 16:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35011</th>\n",
       "      <td>0.38541</td>\n",
       "      <td>0.384861</td>\n",
       "      <td>31/12/2015 16:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35012</th>\n",
       "      <td>0.38540</td>\n",
       "      <td>0.384810</td>\n",
       "      <td>31/12/2015 17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35013</th>\n",
       "      <td>0.38539</td>\n",
       "      <td>0.384755</td>\n",
       "      <td>31/12/2015 17:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35014</th>\n",
       "      <td>0.38538</td>\n",
       "      <td>0.384718</td>\n",
       "      <td>31/12/2015 17:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35015</th>\n",
       "      <td>0.38537</td>\n",
       "      <td>0.384680</td>\n",
       "      <td>31/12/2015 17:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35016</th>\n",
       "      <td>0.38536</td>\n",
       "      <td>0.384642</td>\n",
       "      <td>31/12/2015 18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35017</th>\n",
       "      <td>0.38535</td>\n",
       "      <td>0.384605</td>\n",
       "      <td>31/12/2015 18:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>0.38534</td>\n",
       "      <td>0.384580</td>\n",
       "      <td>31/12/2015 18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35019</th>\n",
       "      <td>0.38533</td>\n",
       "      <td>0.384556</td>\n",
       "      <td>31/12/2015 18:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35020</th>\n",
       "      <td>0.38532</td>\n",
       "      <td>0.384531</td>\n",
       "      <td>31/12/2015 19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35021</th>\n",
       "      <td>0.38531</td>\n",
       "      <td>0.384505</td>\n",
       "      <td>31/12/2015 19:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35022</th>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.384499</td>\n",
       "      <td>31/12/2015 19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35023</th>\n",
       "      <td>0.38529</td>\n",
       "      <td>0.384492</td>\n",
       "      <td>31/12/2015 19:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35024</th>\n",
       "      <td>0.38528</td>\n",
       "      <td>0.384479</td>\n",
       "      <td>31/12/2015 20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35025</th>\n",
       "      <td>0.38527</td>\n",
       "      <td>0.384467</td>\n",
       "      <td>31/12/2015 20:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35026</th>\n",
       "      <td>0.38526</td>\n",
       "      <td>0.384447</td>\n",
       "      <td>31/12/2015 20:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35027</th>\n",
       "      <td>0.38526</td>\n",
       "      <td>0.384427</td>\n",
       "      <td>31/12/2015 20:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35028</th>\n",
       "      <td>0.38525</td>\n",
       "      <td>0.384407</td>\n",
       "      <td>31/12/2015 21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35029</th>\n",
       "      <td>0.38524</td>\n",
       "      <td>0.384388</td>\n",
       "      <td>31/12/2015 21:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35030</th>\n",
       "      <td>0.38523</td>\n",
       "      <td>0.384378</td>\n",
       "      <td>31/12/2015 21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35031</th>\n",
       "      <td>0.38522</td>\n",
       "      <td>0.384369</td>\n",
       "      <td>31/12/2015 21:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35032</th>\n",
       "      <td>0.38521</td>\n",
       "      <td>0.384360</td>\n",
       "      <td>31/12/2015 22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35033</th>\n",
       "      <td>0.38521</td>\n",
       "      <td>0.384351</td>\n",
       "      <td>31/12/2015 22:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35034</th>\n",
       "      <td>0.38520</td>\n",
       "      <td>0.384352</td>\n",
       "      <td>31/12/2015 22:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35035</th>\n",
       "      <td>0.38519</td>\n",
       "      <td>0.384352</td>\n",
       "      <td>31/12/2015 22:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35036</th>\n",
       "      <td>0.38518</td>\n",
       "      <td>0.384353</td>\n",
       "      <td>31/12/2015 23:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35037</th>\n",
       "      <td>0.38517</td>\n",
       "      <td>0.384354</td>\n",
       "      <td>31/12/2015 23:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35038</th>\n",
       "      <td>0.38517</td>\n",
       "      <td>0.384362</td>\n",
       "      <td>31/12/2015 23:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35039</th>\n",
       "      <td>0.38516</td>\n",
       "      <td>0.384369</td>\n",
       "      <td>31/12/2015 23:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23477 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual  Predicted                dt\n",
       "11563  0.34164   0.342362  01/05/2015 10:45\n",
       "11564  0.33944   0.340204  01/05/2015 11:00\n",
       "11565  0.33737   0.338181  01/05/2015 11:15\n",
       "11566  0.33543   0.336288  01/05/2015 11:30\n",
       "11567  0.33362   0.334522  01/05/2015 11:45\n",
       "11568  0.33193   0.332879  01/05/2015 12:00\n",
       "11569  0.33037   0.331339  01/05/2015 12:15\n",
       "11570  0.32892   0.329909  01/05/2015 12:30\n",
       "11571  0.32758   0.328574  01/05/2015 12:45\n",
       "11572  0.32636   0.327350  01/05/2015 13:00\n",
       "11573  0.32524   0.326232  01/05/2015 13:15\n",
       "11574  0.32423   0.325224  01/05/2015 13:30\n",
       "11575  0.32331   0.324319  01/05/2015 13:45\n",
       "11576  0.32250   0.323511  01/05/2015 14:00\n",
       "11577  0.32178   0.322793  01/05/2015 14:15\n",
       "11578  0.32114   0.322170  01/05/2015 14:30\n",
       "11579  0.32060   0.321632  01/05/2015 14:45\n",
       "11580  0.32014   0.321177  01/05/2015 15:00\n",
       "11581  0.31976   0.320803  01/05/2015 15:15\n",
       "11582  0.31946   0.320505  01/05/2015 15:30\n",
       "11583  0.31923   0.320280  01/05/2015 15:45\n",
       "11584  0.31907   0.320125  01/05/2015 16:00\n",
       "11585  0.31898   0.320038  01/05/2015 16:15\n",
       "11586  0.31895   0.320006  01/05/2015 16:30\n",
       "11587  0.31898   0.320035  01/05/2015 16:45\n",
       "11588  0.31907   0.320121  01/05/2015 17:00\n",
       "11589  0.31921   0.320260  01/05/2015 17:15\n",
       "11590  0.31940   0.320424  01/05/2015 17:30\n",
       "11591  0.31964   0.320706  01/05/2015 17:45\n",
       "11592  0.31992   0.321043  01/05/2015 18:00\n",
       "...        ...        ...               ...\n",
       "35010  0.38542   0.384911  31/12/2015 16:30\n",
       "35011  0.38541   0.384861  31/12/2015 16:45\n",
       "35012  0.38540   0.384810  31/12/2015 17:00\n",
       "35013  0.38539   0.384755  31/12/2015 17:15\n",
       "35014  0.38538   0.384718  31/12/2015 17:30\n",
       "35015  0.38537   0.384680  31/12/2015 17:45\n",
       "35016  0.38536   0.384642  31/12/2015 18:00\n",
       "35017  0.38535   0.384605  31/12/2015 18:15\n",
       "35018  0.38534   0.384580  31/12/2015 18:30\n",
       "35019  0.38533   0.384556  31/12/2015 18:45\n",
       "35020  0.38532   0.384531  31/12/2015 19:00\n",
       "35021  0.38531   0.384505  31/12/2015 19:15\n",
       "35022  0.38530   0.384499  31/12/2015 19:30\n",
       "35023  0.38529   0.384492  31/12/2015 19:45\n",
       "35024  0.38528   0.384479  31/12/2015 20:00\n",
       "35025  0.38527   0.384467  31/12/2015 20:15\n",
       "35026  0.38526   0.384447  31/12/2015 20:30\n",
       "35027  0.38526   0.384427  31/12/2015 20:45\n",
       "35028  0.38525   0.384407  31/12/2015 21:00\n",
       "35029  0.38524   0.384388  31/12/2015 21:15\n",
       "35030  0.38523   0.384378  31/12/2015 21:30\n",
       "35031  0.38522   0.384369  31/12/2015 21:45\n",
       "35032  0.38521   0.384360  31/12/2015 22:00\n",
       "35033  0.38521   0.384351  31/12/2015 22:15\n",
       "35034  0.38520   0.384352  31/12/2015 22:30\n",
       "35035  0.38519   0.384352  31/12/2015 22:45\n",
       "35036  0.38518   0.384353  31/12/2015 23:00\n",
       "35037  0.38517   0.384354  31/12/2015 23:15\n",
       "35038  0.38517   0.384362  31/12/2015 23:30\n",
       "35039  0.38516   0.384369  31/12/2015 23:45\n",
       "\n",
       "[23477 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results.to_csv(\"output/fc_output_\" + ROAD  + \"_\" + YEAR + EXT, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "fc.save('models/fc_' + ROAD + '_' + YEAR + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
